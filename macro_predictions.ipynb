{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macro Predictions\n",
    "\n",
    "This notebook generates the lstm prediction for the visualizations in the `Prediction_Visualization` notebook. We first iterate over a list of the countries detailed in teh JHU data, and then over a list of the fifty US staes (abbreviations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing all of the libraries we're going to needs (predominantly pandas and the data loading / inference libaries in this repo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic python libs\n",
    "import os, wget, json\n",
    "\n",
    "# deep learning framework tools\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "# data analysis libs\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "# visualization libs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "# ML pre-processing\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also going to need the predictor and data loading classes in this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ML.lstm_torch import LSTM_data_loader, LSTM_Predictor, train_lstm, predict_future\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid', palette='muted', font_scale=1.6)\n",
    "sns.set_palette(sns.color_palette(\"husl\", 8))\n",
    "\n",
    "rcParams['figure.figsize'] = 16, 12\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1242c37f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set our random seed\n",
    "RANDOM_SEED = 26\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Context and Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two data frames that we ue for these analyses. For the country level data, we can use eithe the most recent data available, or we can use a March 22nd snpshot, which we use for the US-state level analysis.\n",
    "\n",
    "The pro and cons are fairly straightforward; the newer data provides more, better information as earlier recorded dates are fairly sparse. We can see this in lower recorded training and testing Mean Squared Errors for our predictins. That said, after Macrh 22nd, state-level information was dropped from the data, so tarining models at that level of granularity is nog longer possible uunless we use deprecated data. For this analysis, and the analysis in the visual, we use the March 22nd snap shop. If you wish to do otherwise, though, swicth out `./data/jhu_data/time_series_19-covid-Confirmed_3_22.csv` for `time_series_covid19_confirmed_global.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time_series_covid19_confirmed_global.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.isfile('time_series_covid19_confirmed_global.csv'):\n",
    "    os.remove('time_series_covid19_confirmed_global.csv')\n",
    "    \n",
    "wget.download('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = pd.read_csv('./data/jhu_data/time_series_19-covid-Confirmed_3_22.csv')\n",
    "tdf.drop(columns='Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Province/State</th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>1/22/20</th>\n",
       "      <th>1/23/20</th>\n",
       "      <th>1/24/20</th>\n",
       "      <th>1/25/20</th>\n",
       "      <th>1/26/20</th>\n",
       "      <th>1/27/20</th>\n",
       "      <th>...</th>\n",
       "      <th>3/13/20</th>\n",
       "      <th>3/14/20</th>\n",
       "      <th>3/15/20</th>\n",
       "      <th>3/16/20</th>\n",
       "      <th>3/17/20</th>\n",
       "      <th>3/18/20</th>\n",
       "      <th>3/19/20</th>\n",
       "      <th>3/20/20</th>\n",
       "      <th>3/21/20</th>\n",
       "      <th>3/22/20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Thailand</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>101.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>75</td>\n",
       "      <td>82</td>\n",
       "      <td>114</td>\n",
       "      <td>147</td>\n",
       "      <td>177</td>\n",
       "      <td>212</td>\n",
       "      <td>272</td>\n",
       "      <td>322</td>\n",
       "      <td>411</td>\n",
       "      <td>599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Japan</td>\n",
       "      <td>36.0000</td>\n",
       "      <td>138.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>701</td>\n",
       "      <td>773</td>\n",
       "      <td>839</td>\n",
       "      <td>825</td>\n",
       "      <td>878</td>\n",
       "      <td>889</td>\n",
       "      <td>924</td>\n",
       "      <td>963</td>\n",
       "      <td>1007</td>\n",
       "      <td>1086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>1.2833</td>\n",
       "      <td>103.8333</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>200</td>\n",
       "      <td>212</td>\n",
       "      <td>226</td>\n",
       "      <td>243</td>\n",
       "      <td>266</td>\n",
       "      <td>313</td>\n",
       "      <td>345</td>\n",
       "      <td>385</td>\n",
       "      <td>432</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>28.1667</td>\n",
       "      <td>84.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>2.5000</td>\n",
       "      <td>112.5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>197</td>\n",
       "      <td>238</td>\n",
       "      <td>428</td>\n",
       "      <td>566</td>\n",
       "      <td>673</td>\n",
       "      <td>790</td>\n",
       "      <td>900</td>\n",
       "      <td>1030</td>\n",
       "      <td>1183</td>\n",
       "      <td>1306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Province/State Country/Region      Lat      Long  1/22/20  1/23/20  1/24/20  \\\n",
       "0            NaN       Thailand  15.0000  101.0000        2        3        5   \n",
       "1            NaN          Japan  36.0000  138.0000        2        1        2   \n",
       "2            NaN      Singapore   1.2833  103.8333        0        1        3   \n",
       "3            NaN          Nepal  28.1667   84.2500        0        0        0   \n",
       "4            NaN       Malaysia   2.5000  112.5000        0        0        0   \n",
       "\n",
       "   1/25/20  1/26/20  1/27/20   ...     3/13/20  3/14/20  3/15/20  3/16/20  \\\n",
       "0        7        8        8   ...          75       82      114      147   \n",
       "1        2        4        4   ...         701      773      839      825   \n",
       "2        3        4        5   ...         200      212      226      243   \n",
       "3        1        1        1   ...           1        1        1        1   \n",
       "4        3        4        4   ...         197      238      428      566   \n",
       "\n",
       "   3/17/20  3/18/20  3/19/20  3/20/20  3/21/20  3/22/20  \n",
       "0      177      212      272      322      411      599  \n",
       "1      878      889      924      963     1007     1086  \n",
       "2      266      313      345      385      432      455  \n",
       "3        1        1        1        1        1        2  \n",
       "4      673      790      900     1030     1183     1306  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in our state geojson data to make our mapping dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/geo_data/us-states.json', 'r') as f:\n",
    "    us_states = json.load(f)\n",
    "    \n",
    "state_abrs = [x['id'] for x in us_states['features']]\n",
    "\n",
    "state_mapper_lst = [{x['properties']['name']:x['id']} for x in us_states['features']]\n",
    "\n",
    "state_mapper_lst\n",
    "state_mapper = {}\n",
    "for s in state_mapper_lst:\n",
    "    state_mapper.update(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we can run through using the NY abbreviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is converted to daily delta\n",
      "Epoch 0 train loss: 1.6421136856079102\n",
      "Epoch 50 train loss: 0.8585584759712219\n",
      "Epoch 100 train loss: 0.48346182703971863\n",
      "Epoch 150 train loss: 0.48300594091415405\n",
      "Epoch 200 train loss: 0.4827078580856323\n",
      "Epoch 250 train loss: 0.482661634683609\n",
      "[3003, 3665, 4670, 6008, 7720, 9850, 12416, 15372, 18580, 21824]\n"
     ]
    }
   ],
   "source": [
    "state_data_loader = LSTM_data_loader(df=tdf,\n",
    "                                       region_abr='NY',\n",
    "                                       country='US',\n",
    "                                       state_mapper=state_mapper)\n",
    "\n",
    "state_data_loader.subset_df()\n",
    "state_data_loader.transform_df_datetime(delta=True)\n",
    "\n",
    "state_data_loader.gen_data_sets(test_data_size=0)\n",
    "\n",
    "X_train, y_train = state_data_loader.set_seq(train=True, sequence_lenth=3)\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).float()\n",
    "\n",
    "model = LSTM_Predictor(features=1,\n",
    "                       neurons=512,\n",
    "                       sequences=3,\n",
    "                       layers=2,\n",
    "                       dropout=0.0)\n",
    "\n",
    "model, _, _ = train_lstm(model,\n",
    "                                      X_train,\n",
    "                                      y_train,\n",
    "                                      epochs=300)\n",
    "\n",
    "seq_length = model.sequences\n",
    "days_to_predict = 10\n",
    "\n",
    "outs = predict_future(n_future=days_to_predict, \n",
    "                      time_data=X_train, \n",
    "                      sequece_lenth=model.sequences, \n",
    "                      model=model)\n",
    "\n",
    "predicted_cases = state_data_loader.scaler.inverse_transform(\n",
    "  np.expand_dims(outs, axis=0)\n",
    ").flatten()\n",
    "print([int(x) for x in predicted_cases.tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Data For Viz\n",
    "\n",
    "The below for loop takes the process that we just did for New York and performs it for every state in the US. We take the trained models and predict the next ten days, and write these predictions to a json. We then take these results and visualize them in the results viz notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_saver_nodelta = {}\n",
    "\n",
    "counter = 1\n",
    "\n",
    "for state in state_abrs:\n",
    "    \n",
    "    print('{}: {} out of {}'.format(state, counter, len(state_abrs)))\n",
    "    \n",
    "    state_data_loader = LSTM_data_loader(df=tdf,\n",
    "                                           region_abr=state,\n",
    "                                           country='US',\n",
    "                                           state_mapper=state_mapper)\n",
    "    \n",
    "    state_data_loader.subset_df()\n",
    "\n",
    "    state_data_loader.transform_df_datetime(delta=False)\n",
    "\n",
    "    state_data_loader.gen_data_sets(test_data_size=0)\n",
    "    \n",
    "    X_train, y_train = state_data_loader.set_seq(train=True, sequence_length=3)\n",
    "    X_train = torch.from_numpy(X_train).float()\n",
    "    y_train = torch.from_numpy(y_train).float()\n",
    "    \n",
    "    model = LSTM_Predictor(features=1,\n",
    "                           neurons=512,\n",
    "                           sequences=3,\n",
    "                           layers=2,\n",
    "                           dropout=0.3)\n",
    "\n",
    "    model, train_hist, test_hist = train_lstm(model,\n",
    "                                          X_train,\n",
    "                                          y_train,\n",
    "                                          epochs=300)\n",
    "    \n",
    "    seq_length = model.sequences\n",
    "    days_to_predict = 10\n",
    "\n",
    "    outs = predict_future(n_future=days_to_predict, \n",
    "                          time_data=X_train, \n",
    "                          sequece_lenth=model.sequences, \n",
    "                          model=model)\n",
    "\n",
    "    predicted_cases = state_data_loader.scaler.inverse_transform(\n",
    "      np.expand_dims(outs, axis=0)\n",
    "    ).flatten()\n",
    "    print(predicted_cases)\n",
    "    \n",
    "    data_saver_nodelta[state] = predicted_cases\n",
    "    counter+=1\n",
    "    \n",
    "    data_try = data_saver_nodelta.copy()\n",
    "\n",
    "    for key in data_try.keys():\n",
    "        data_try[key] = data_try[key].tolist()\n",
    "\n",
    "    for_writing = json.dumps(data_try)\n",
    "    \n",
    "    with open('states_predictions.json', 'w') as fp:\n",
    "            json.dump(for_writing, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Country Predictions\n",
    "\n",
    "We then do the same process, but instead of processing 50 states, we process each country recorded in the JHU data. Additonally, this model is trained on new daily cases, as opposed to the total confirmed cases in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501\n",
      "183\n"
     ]
    }
   ],
   "source": [
    "country_list = tdf['Country/Region'].tolist()\n",
    "print(len(country_list))\n",
    "country_list = list(dict.fromkeys(country_list))\n",
    "print(len(country_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_saver = {}\n",
    "\n",
    "counter = 1\n",
    "\n",
    "for country in country_list:\n",
    "    \n",
    "    print('{}: {} out of {}'.format(country, counter, len(country_list)))\n",
    "    \n",
    "    country_data_loader = LSTM_data_loader(df=tdf,\n",
    "                                           region_abr=None,\n",
    "                                           country=country,\n",
    "                                           state_mapper=None)\n",
    "    \n",
    "    country_data_loader.subset_df()\n",
    "\n",
    "    country_data_loader.transform_df_datetime(delta=False)\n",
    "    try:\n",
    "        country_data_loader.gen_data_sets(test_data_size=0)\n",
    "\n",
    "        X_train, y_train = country_data_loader.set_seq(train=True,sequence_lenght=3)\n",
    "        X_train = torch.from_numpy(X_train).float()\n",
    "        y_train = torch.from_numpy(y_train).float()\n",
    "\n",
    "        model = LSTM_Predictor(features=1,\n",
    "                               neurons=512,\n",
    "                               sequences=3,\n",
    "                               layers=2,\n",
    "                               dropout=0.3)\n",
    "\n",
    "        model, _, _ = train_lstm(model,\n",
    "                                              X_train,\n",
    "                                              y_train,\n",
    "                                              epochs=300)\n",
    "\n",
    "        seq_length = model.sequences\n",
    "        days_to_predict = 10\n",
    "\n",
    "        outs = predict_future(n_future=days_to_predict, \n",
    "                              time_data=X_train, \n",
    "                              sequece_lenth=model.sequences, \n",
    "                              model=model)\n",
    "\n",
    "        predicted_cases = country_data_loader.scaler.inverse_transform(\n",
    "          np.expand_dims(outs, axis=0)\n",
    "        ).flatten()\n",
    "        print(predicted_cases)\n",
    "\n",
    "        data_saver[country] = predicted_cases\n",
    "        counter+=1\n",
    "\n",
    "        data_try = data_saver.copy()\n",
    "\n",
    "        for key in data_try.keys():\n",
    "            data_try[key] = data_try[key].tolist()\n",
    "\n",
    "        for_writing = json.dumps(data_try)\n",
    "\n",
    "        with open('country_predictions_delta.json', 'w') as fp:\n",
    "            json.dump(for_writing, fp)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
